import os
import gradio as gr
from dotenv import load_dotenv
from tianji.knowledges.langchain_onlinellm.models import SiliconFlowEmbeddings, SiliconFlowLLM
from langchain_chroma import Chroma
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain import hub
from tianji import TIANJI_PATH
import argparse
from huggingface_hub import snapshot_download
import requests
import loguru
load_dotenv()

parser = argparse.ArgumentParser(description='Launch Gradio application')
parser.add_argument('--listen', action='store_true', help='Specify to listen on 0.0.0.0')
parser.add_argument('--port', type=int, default=None, help='The port the server should listen on')
parser.add_argument('--root_path', type=str, default=None, help='The root path of the server')
parser.add_argument('--force', action='store_true', help='Force recreate the database')
parser.add_argument('--chunk_size', type=int, default=896, help='Chunk size for text splitting')
args = parser.parse_args()

# å¼€å§‹å‰æ£€æŸ¥åŠŸèƒ½æ˜¯å¦æ­£å¸¸
try:
    llm = SiliconFlowLLM()
    test_response = llm._call("ä½ å¥½")
    loguru.logger.info("SiliconFlowèŠå¤©åŠŸèƒ½æµ‹è¯•æˆåŠŸ")
except Exception as e:
    loguru.logger.error("SiliconFlowèŠå¤©åŠŸèƒ½æµ‹è¯•å¤±è´¥: {}", str(e))
    raise e
try:
    embeddings = SiliconFlowEmbeddings()
    test_text = "æµ‹è¯•æ–‡æœ¬"
    test_embedding = embeddings.embed_query(test_text)
    if len(test_embedding) > 0:
        loguru.logger.info("SiliconFlowåµŒå…¥åŠŸèƒ½æµ‹è¯•æˆåŠŸ")
    else:
        raise ValueError("åµŒå…¥å‘é‡é•¿åº¦ä¸º0")
except Exception as e:
    loguru.logger.error("SiliconFlowåµŒå…¥åŠŸèƒ½æµ‹è¯•å¤±è´¥: {}", str(e))
    raise e

# æ­£å¼ä»£ç 
def check_internet_connection(url='http://www.google.com/', timeout=5):
    try:
        _ = requests.head(url, timeout=timeout)
        return True
    except requests.ConnectionError:
        return False
    
destination_folder = os.path.join(TIANJI_PATH, "temp", "tianji-chinese")
if not os.path.exists(destination_folder):
    if not check_internet_connection():
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    for _ in range(5):
        try:
            snapshot_download(
                repo_id="sanbu/tianji-chinese",
                local_dir=destination_folder,
                repo_type="dataset",
                local_dir_use_symlinks=False,
                endpoint=os.environ.get('HF_ENDPOINT', None),
            )
            break
        except Exception as e:
            loguru.logger.error("Download failed, retrying... Error message: {}", str(e))
    else:
        loguru.logger.error("Download failed, maximum retry count reached.")


def create_vectordb(
    data_path: str,
    persist_directory: str,
    embedding_func,
    chunk_size: int,
    force: bool = False,
):
    if os.path.exists(persist_directory) and not force:
        return Chroma(
            persist_directory=persist_directory, embedding_function=embedding_func
        )
    if force and os.path.exists(persist_directory):
        if os.path.isdir(persist_directory):
            import shutil
            shutil.rmtree(persist_directory)
        else:
            os.remove(persist_directory)
    # ä½¿ç”¨ UTF-8 ç¼–ç åŠ è½½æ–‡ä»¶ï¼Œé¿å… Windows ç³»ç»Ÿä¸Šçš„ç¼–ç é—®é¢˜
    loader = DirectoryLoader(
        data_path, 
        glob="*.txt", 
        loader_cls=TextLoader,
        loader_kwargs={"encoding": "utf-8"}
    )
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=200
    )
    split_docs = text_splitter.split_documents(loader.load())
    if len(split_docs) == 0:
        loguru.logger.error("Invalid knowledge data, processing data results in empty, check if data download failed, can be downloaded manually")
        raise gr.Error("Invalid knowledge data, processing data results in empty, check if data download failed, can be downloaded manually")
    
    # å¤„ç† API é€Ÿç‡é™åˆ¶ï¼šåˆ†æ‰¹å¤„ç†æ–‡æ¡£å¹¶æ·»åŠ å»¶è¿Ÿ
    loguru.logger.info("å¼€å§‹åˆ›å»ºå‘é‡æ•°æ®åº“ï¼Œæ–‡æ¡£æ•°é‡: {}", len(split_docs))
    try:
        # åˆ†æ‰¹å¤„ç†æ–‡æ¡£ä»¥é¿å…é€Ÿç‡é™åˆ¶
        batch_size = 10  # æ¯æ‰¹å¤„ç†10ä¸ªæ–‡æ¡£
        import time
        
        # å¦‚æœæ–‡æ¡£æ•°é‡è¾ƒå¤šï¼Œåˆ†æ‰¹æ·»åŠ 
        if len(split_docs) > batch_size:
            loguru.logger.info("æ–‡æ¡£æ•°é‡è¾ƒå¤šï¼Œå°†åˆ†æ‰¹å¤„ç†ä»¥é¿å…é€Ÿç‡é™åˆ¶...")
            # å…ˆåˆ›å»ºç©ºçš„å‘é‡æ•°æ®åº“
            vector_db = Chroma(
                embedding_function=embedding_func,
                persist_directory=persist_directory,
            )
            
            # åˆ†æ‰¹æ·»åŠ æ–‡æ¡£
            for i in range(0, len(split_docs), batch_size):
                batch = split_docs[i:i + batch_size]
                loguru.logger.info("å¤„ç†æ‰¹æ¬¡ {}/{} (å…± {} ä¸ªæ–‡æ¡£)", 
                                  i // batch_size + 1, 
                                  (len(split_docs) + batch_size - 1) // batch_size,
                                  len(batch))
                try:
                    vector_db.add_documents(batch)
                    # æ·»åŠ å»¶è¿Ÿä»¥é¿å…é€Ÿç‡é™åˆ¶ï¼ˆæ¯åˆ†é’Ÿè¯·æ±‚æ•°é™åˆ¶ï¼‰
                    if i + batch_size < len(split_docs):
                        time.sleep(1)  # æ¯æ‰¹ä¹‹é—´å»¶è¿Ÿ1ç§’
                except Exception as batch_error:
                    error_msg = str(batch_error)
                    if "RPM limit exceeded" in error_msg or "403" in error_msg:
                        loguru.logger.warning("é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…60ç§’åé‡è¯•...")
                        time.sleep(60)  # ç­‰å¾…60ç§’
                        vector_db.add_documents(batch)  # é‡è¯•
                    else:
                        raise batch_error
        else:
            # æ–‡æ¡£æ•°é‡å°‘ï¼Œç›´æ¥åˆ›å»º
            vector_db = Chroma.from_documents(
                documents=split_docs,
                embedding=embedding_func,
                persist_directory=persist_directory,
            )
        loguru.logger.info("å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        error_msg = str(e)
        loguru.logger.error("åˆ›å»ºæ•°æ®åº“å¤±è´¥: {}", error_msg)
        if "RPM limit exceeded" in error_msg or "403" in error_msg:
            loguru.logger.error("API é€Ÿç‡é™åˆ¶å·²è¶…å‡ºã€‚è¯·:")
            loguru.logger.error("1. ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•")
            loguru.logger.error("2. å®Œæˆ SiliconFlow èº«ä»½éªŒè¯ä»¥è§£é™¤é™åˆ¶")
            loguru.logger.error("3. ä½¿ç”¨ --force å‚æ•°æ—¶ï¼Œå¯ä»¥åˆ†æ‰¹è¿è¡Œæˆ–ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹")
            raise gr.Error(
                "API é€Ÿç‡é™åˆ¶å·²è¶…å‡ºã€‚\n"
                "è§£å†³æ–¹æ¡ˆï¼š\n"
                "1. ç­‰å¾…1-2åˆ†é’Ÿåé‡è¯•\n"
                "2. å®Œæˆ SiliconFlow èº«ä»½éªŒè¯ä»¥è§£é™¤é™åˆ¶\n"
                "3. å¦‚æœå·²æœ‰æ•°æ®åº“ï¼Œä¸ä½¿ç”¨ --force å‚æ•°ç›´æ¥è¿è¡Œ"
            )
        raise e
    return vector_db


def initialize_chain(chunk_size: int, persist_directory: str, data_path: str, force=False):
    loguru.logger.info("åˆå§‹åŒ–æ•°æ®åº“å¼€å§‹ï¼Œå½“å‰æ•°æ®è·¯å¾„ä¸ºï¼š{}", data_path)
    vectordb = create_vectordb(data_path, persist_directory, embeddings, chunk_size, force)
    retriever = vectordb.as_retriever()
    prompt = hub.pull("rlm/rag-prompt")
    prompt.messages[
        0
    ].prompt.template = """
    æ‚¨æ˜¯ä¸€åç”¨äºé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœæ²¡æœ‰é«˜åº¦ç›¸å…³ä¸Šä¸‹æ–‡ ä½ å°±è‡ªç”±å›ç­”ã€‚\
    æ ¹æ®æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œç»“åˆæˆ‘çš„é—®é¢˜,ç›´æ¥ç»™å‡ºæœ€åçš„å›ç­”ï¼Œè¦åªç´§æ‰£é—®é¢˜å›´ç»•ç€å›ç­”ï¼Œå°½é‡æ ¹æ®æ¶‰åŠå‡ ä¸ªå…³é”®ç‚¹ç”¨å®Œæ•´éå¸¸è¯¦ç»†çš„å‡ æ®µè¯å›å¤ã€‚ã€‚\
    \né—®é¢˜ï¼š{question} \nä¸Šä¸‹æ–‡ï¼š{context} \nå›ç­”ï¼š
    """
    loguru.logger.info("åˆå§‹åŒ–æ•°æ®åº“ç»“æŸ")
    return (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


def handle_question(chain, question: str, chat_history):
    if not question:
        return "", chat_history
    if chat_history is None:
        chat_history = []
    try:
        result = chain.invoke(question)
        # ä½¿ç”¨å­—å…¸æ ¼å¼å…¼å®¹æ–°ç‰ˆæœ¬ Gradio
        chat_history.append({"role": "user", "content": question})
        chat_history.append({"role": "assistant", "content": result})
        return "", chat_history
    except Exception as e:
        loguru.logger.error("å¤„ç†é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {}", str(e))
        error_msg = str(e)
        chat_history.append({"role": "user", "content": question})
        chat_history.append({"role": "assistant", "content": f"é”™è¯¯: {error_msg}"})
        return "", chat_history


# Define scenarios
scenarios = {
    "æ•¬é…’ç¤¼ä»ªæ–‡åŒ–": "1-etiquette",
    "è¯·å®¢ç¤¼ä»ªæ–‡åŒ–": "2-hospitality",
    "é€ç¤¼ç¤¼ä»ªæ–‡åŒ–": "3-gifting",
    "å¦‚ä½•è¯´å¯¹è¯": "5-communication",
    "åŒ–è§£å°´å°¬åœºåˆ": "6-awkwardness",
    "çŸ›ç›¾&å†²çªåº”å¯¹": "7-conflict",
}

# Initialize chains for all scenarios
chains = {}
loguru.logger.info("=" * 60)
loguru.logger.info("å¼€å§‹åˆå§‹åŒ–æ‰€æœ‰åœºæ™¯çš„çŸ¥è¯†åº“...")
loguru.logger.info("æç¤ºï¼šå¦‚æœé‡åˆ°é€Ÿç‡é™åˆ¶é”™è¯¯ï¼Œå¯ä»¥ï¼š")
loguru.logger.info("1. ç­‰å¾…1-2åˆ†é’Ÿåé‡è¯•ï¼ˆä¸ä½¿ç”¨ --forceï¼‰")
loguru.logger.info("2. å®Œæˆ SiliconFlow èº«ä»½éªŒè¯ä»¥è§£é™¤é™åˆ¶")
loguru.logger.info("3. å¦‚æœæ•°æ®åº“å·²å­˜åœ¨ï¼Œç›´æ¥è¿è¡Œï¼ˆä¸ä½¿ç”¨ --forceï¼‰")
loguru.logger.info("=" * 60)

for scenario_name, scenario_folder in scenarios.items():
    data_path = os.path.join(
        TIANJI_PATH, "temp", "tianji-chinese", "RAG", scenario_folder
    )
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"Data path does not exist: {data_path}")

    persist_directory = os.path.join(TIANJI_PATH, "temp", f"chromadb_{scenario_folder}")
    loguru.logger.info("æ­£åœ¨åˆå§‹åŒ–åœºæ™¯: {} ({})", scenario_name, scenario_folder)
    try:
        chains[scenario_name] = initialize_chain(args.chunk_size, persist_directory, data_path, args.force)
        loguru.logger.info("åœºæ™¯ {} åˆå§‹åŒ–æˆåŠŸ", scenario_name)
    except Exception as e:
        error_msg = str(e)
        if "RPM limit exceeded" in error_msg or "403" in error_msg:
            loguru.logger.error("åœºæ™¯ {} åˆå§‹åŒ–å¤±è´¥ï¼šAPI é€Ÿç‡é™åˆ¶", scenario_name)
            loguru.logger.error("è¯·ç­‰å¾…åé‡è¯•ï¼Œæˆ–å®Œæˆèº«ä»½éªŒè¯")
            # å¦‚æœå…¶ä»–åœºæ™¯å·²åˆå§‹åŒ–ï¼Œç»§ç»­è¿è¡Œï¼›å¦åˆ™é€€å‡º
            if len(chains) == 0:
                raise e
        else:
            raise e

loguru.logger.info("æ‰€æœ‰åœºæ™¯åˆå§‹åŒ–å®Œæˆï¼Œå…± {} ä¸ªåœºæ™¯", len(chains))

# Create Gradio interface
TITLE = """
# Tianji äººæƒ…ä¸–æ•…å¤§æ¨¡å‹ç³»ç»Ÿå®Œæ•´ç‰ˆ(åŸºäºçŸ¥è¯†åº“å®ç°) æ¬¢è¿starï¼\n
## ğŸ’«å¼€æºé¡¹ç›®åœ°å€ï¼šhttps://github.com/SocialAI-tianji/Tianji
## ä½¿ç”¨æ–¹æ³•ï¼šé€‰æ‹©ä½ æƒ³æé—®çš„åœºæ™¯ï¼Œè¾“å…¥æç¤ºï¼Œæˆ–ç‚¹å‡»Exampleè‡ªåŠ¨å¡«å……
## å¦‚æœè§‰å¾—å›ç­”ä¸æ»¡æ„,å¯è¡¥å……æ›´å¤šä¿¡æ¯é‡å¤æé—®ã€‚
### æˆ‘ä»¬çš„æ„¿æ™¯æ˜¯æ„å»ºä¸€ä¸ªä»æ•°æ®æ”¶é›†å¼€å§‹çš„å¤§æ¨¡å‹å…¨æ ˆå‚ç›´é¢†åŸŸå¼€æºå®è·µ.
"""


def get_examples_for_scenario(scenario):
    # Define examples for each scenario
    examples_dict = {
        "æ•¬é…’ç¤¼ä»ªæ–‡åŒ–": [
            "å–é…’åº§ä½æ€ä¹ˆæ’",
            "å–é…’çš„å…ˆåé¡ºåºæµç¨‹æ˜¯ä»€ä¹ˆ",
            "å–é…’éœ€è¦æ³¨æ„ä»€ä¹ˆ",
            "æ¨èçš„æ•¬é…’è¯æ€ä¹ˆè¯´",
            "å®´ä¼šæ€ä¹ˆç‚¹èœ",
            "å–é…’å®¹æ˜“é†‰æ€ä¹ˆåŠ",
            "å–é…’çš„è§„çŸ©æ˜¯ä»€ä¹ˆ",
        ],
        "è¯·å®¢ç¤¼ä»ªæ–‡åŒ–": ["è¯·å®¢æœ‰é‚£äº›è§„çŸ©", "å¦‚ä½•é€‰æ‹©åˆé€‚çš„é¤å…", "æ€ä¹ˆè¯·åˆ«äººåƒé¥­"],
        "é€ç¤¼ç¤¼ä»ªæ–‡åŒ–": ["é€ä»€ä¹ˆç¤¼ç‰©ç»™é•¿è¾ˆå¥½", "æ€ä¹ˆé€ç¤¼", "å›ç¤¼çš„ç¤¼èŠ‚æ˜¯ä»€ä¹ˆ"],
        "å¦‚ä½•è¯´å¯¹è¯": [
            "æ€ä¹ˆå’Œå¯¼å¸ˆæ²Ÿé€š",
            "æ€ä¹ˆæé«˜æƒ…å•†",
            "å¦‚ä½•è¯»æ‡‚æ½œå°è¯",
            "æ€ä¹ˆå®‰æ…°åˆ«äºº",
            "æ€ä¹ˆå’Œå­©å­æ²Ÿé€š",
            "å¦‚ä½•ä¸ç”·ç”ŸèŠå¤©",
            "å¦‚ä½•ä¸å¥³ç”ŸèŠå¤©",
            "èŒåœºé«˜æƒ…å•†å›åº”æŠ€å·§",
        ],
        "åŒ–è§£å°´å°¬åœºåˆ": ["æ€ä¹ˆå›åº”èµç¾", "æ€ä¹ˆæ‹’ç»å€Ÿé’±", "å¦‚ä½•é«˜æ•ˆæ²Ÿé€š", "æ€ä¹ˆå’Œå¯¹è±¡æ²Ÿé€š", "èŠå¤©æŠ€å·§", "æ€ä¹ˆæ‹’ç»åˆ«äºº", "èŒåœºæ€ä¹ˆæ²Ÿé€š"],
        "çŸ›ç›¾&å†²çªåº”å¯¹": [
            "æ€ä¹ˆæ§åˆ¶æƒ…ç»ª",
            "æ€ä¹ˆå‘åˆ«äººé“æ­‰",
            "å’Œåˆ«äººåµæ¶äº†æ€ä¹ˆåŠ",
            "å¦‚ä½•åŒ–è§£å°´å°¬",
            "å­©å­æœ‰æƒ…ç»ªæ€ä¹ˆåŠ",
            "å¤«å¦»åµæ¶æ€ä¹ˆåŠ",
            "æƒ…ä¾£å†·æˆ˜æ€ä¹ˆåŠ",
        ],
    }
    return examples_dict.get(scenario, [])


with gr.Blocks() as demo:
    gr.Markdown(TITLE)

    init_status = gr.Textbox(label="åˆå§‹åŒ–çŠ¶æ€", value="æ•°æ®åº“å·²åˆå§‹åŒ–", interactive=False)

    with gr.Tabs() as tabs:
        for scenario_name in scenarios.keys():
            with gr.Tab(scenario_name):
                chatbot = gr.Chatbot(height=450)
                msg = gr.Textbox(label="è¾“å…¥ä½ çš„ç–‘é—®")

                examples = gr.Examples(
                    label="å¿«é€Ÿç¤ºä¾‹",
                    examples=get_examples_for_scenario(scenario_name),
                    inputs=[msg],
                )

                with gr.Row():
                    chat_button = gr.Button("èŠå¤©")
                    clear_button = gr.ClearButton(components=[chatbot], value="æ¸…é™¤èŠå¤©è®°å½•")

                # Define a function to invoke the chain for the current scenario
                def invoke_chain(question, chat_history, scenario=scenario_name):
                    loguru.logger.info(question)
                    return handle_question(chains[scenario], question, chat_history)

                chat_button.click(
                    invoke_chain,
                    inputs=[msg, chatbot],
                    outputs=[msg, chatbot],
                )


if __name__ == "__main__":
    server_name = '0.0.0.0' if args.listen else None
    server_port = args.port
    demo.launch(server_name=server_name, server_port=server_port, root_path=args.root_path)
